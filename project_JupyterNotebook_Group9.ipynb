{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 0: Import all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Write all functions needed to execute the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: filename to a json file containing the desired dictionary\n",
    "# Output: Return the dictionary obtained from the file\n",
    "# Read in all json files created from the datacollection.py script\n",
    "# json files contain dictionary of {review1: rating1, review2: rating2, ... , reviewn: ratingn}\n",
    "def getjsons(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        return json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a movie DataFrame\n",
    "# Output: Return an array containing the sentiments from the ratings\n",
    "# Sentiment is obtained by looping through all of the ratings\n",
    "# 3.0 stars or above is given \"Positive\" and 2.5 stars or below is given \"Negative\"\n",
    "def getsentiment(df):\n",
    "    sentiments = []\n",
    "    for val in df['Rating']:\n",
    "        if float(val) >= 3.0:\n",
    "            sentiments.append(\"Positive\")\n",
    "        else:\n",
    "            sentiments.append(\"Negative\")\n",
    "    return np.array(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: dictionary containing reviews mapped to ratings\n",
    "# Output: Return a Pandas DataFrame containing reviews in column 1, ratings in column 2, sentiment in column 3\n",
    "# Sentiment is obtained by the getsentiment function\n",
    "def createdf(dictionary):\n",
    "    df = pd.DataFrame(dictionary.items(), columns = ['Review', 'Rating'], dtype=None)\n",
    "    df['Sentiment'] = getsentiment(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a movie DataFrame and the name of the movie\n",
    "# Output: Obtain and print out the 20 most common words from each given movie\n",
    "def mostcommon(df, name):\n",
    "    # Initialize empty counts array (will contain dictionary of counts to their words)\n",
    "    counts = []\n",
    "    # Initialize a CountVectorizer\n",
    "    vector = CountVectorizer()\n",
    "    # Create a DataFrame containing words and their counts\n",
    "    words = df['Review']\n",
    "    dtm = vector.fit(words.values.astype('U'))\n",
    "    names = dtm.get_feature_names()\n",
    "    dtm = vector.fit_transform(words.values.astype('U'))\n",
    "    dtm_dense = dtm.todense()\n",
    "    df = pd.DataFrame(dtm_dense, columns=names)\n",
    "    for col in df:\n",
    "        counts.append((df[col].sum(), col))\n",
    "    print \"\\nTop 20 words to describe\", name\n",
    "    # Sort the dictionary by count and print out word correlated to count in top 20 counts\n",
    "    for i in sorted(counts)[-20:]:\n",
    "        print i[1], \":\", i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: training and testing data\n",
    "# Output: returns the accuracy score obtained from naive bayes algorithm\n",
    "def naive_bayes(train, test):\n",
    "    # Map positive as 1, negative as 0 for training and testing data\n",
    "    train['Label_num'] = train.Sentiment.map({'Positive':1, 'Negative':0})\n",
    "    test['Label_num'] = test.Sentiment.map({'Positive':1, 'Negative':0})\n",
    "\n",
    "    # X (train and test) will be the review, y will be positive or negative\n",
    "    X_train = train.Review\n",
    "    y_train = train.Label_num\n",
    "    X_test = test.Review\n",
    "    y_test = test.Label_num\n",
    "    \n",
    "    # Initialize a CountVectorizer\n",
    "    vector = CountVectorizer()\n",
    "    # Create document-term matrices (note: astype('U') deals with strange unicode characters)\n",
    "    X_train_dtm = vector.fit_transform(X_train.values.astype('U'))\n",
    "    X_test_dtm = vector.transform(X_test.values.astype('U'))\n",
    "\n",
    "    # Initialize a MultinomialNB object\n",
    "    nb = MultinomialNB()\n",
    "    # Fit the data\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    # Predict values with X testing data\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "    # Grab accuracy score and return\n",
    "    score = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: training and testing data\n",
    "# Output: returns the accuracy score obtained from logistic regression algorithm\n",
    "def log_reg(train, test):\n",
    "    # Map positive to 1, negative to 0 for training and testing data\n",
    "    train['Label_num'] = train.Sentiment.map({'Positive':1, 'Negative':0})\n",
    "    test['Label_num'] = test.Sentiment.map({'Positive':1, 'Negative':0})\n",
    "\n",
    "    # X (train and test) is the review, y is the positive or negative sentiment\n",
    "    X_train = train.Review\n",
    "    y_train = train.Label_num\n",
    "    X_test = test.Review\n",
    "    y_test = test.Label_num\n",
    "    \n",
    "    # Initailize CountVectorizer object and create document-term matrices (note: astype('U') handles strange unicode characters)\n",
    "    vector = CountVectorizer()\n",
    "    X_train_dtm = vector.fit_transform(X_train.values.astype('U'))\n",
    "    X_test_dtm = vector.transform(X_test.values.astype('U'))\n",
    "    \n",
    "    # Initialize LogisticRegression object and fit the data\n",
    "    logistic = LogisticRegression()\n",
    "    logistic.fit(X_train_dtm, y_train)\n",
    "    \n",
    "    # Obtain predicted result and return the accuracy score\n",
    "    y_predicted_class = logistic.predict(X_test_dtm)\n",
    "    return metrics.accuracy_score(y_test, y_predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Grab all data from files, preprocess the data into training and testing DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use getjsons function to obtain dictionaries for all movies where {review1: rating1, ... , reviewn: ratingn}\n",
    "starwars = getjsons(\"starwars.json\")\n",
    "abouttime = getjsons(\"abouttime.json\")\n",
    "taken = getjsons(\"taken.json\")\n",
    "toystory = getjsons(\"toystory.json\")\n",
    "cloudatlas = getjsons(\"cloudatlas.json\")\n",
    "stepbrothers = getjsons(\"stepbrothers.json\")\n",
    "saw = getjsons(\"saw.json\")\n",
    "saw2 = getjsons(\"saw2.json\")\n",
    "titanic = getjsons(\"titanic.json\")\n",
    "piratesofthecaribbean = getjsons(\"piratesofthecaribbean.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call createdf function on all dictionaries\n",
    "# DataFrames will have columns of Reviews, Ratings, and Sentiment\n",
    "starwars = createdf(starwars)\n",
    "abouttime = createdf(abouttime)\n",
    "taken = createdf(taken)\n",
    "toystory = createdf(toystory)\n",
    "cloudatlas = createdf(cloudatlas)\n",
    "stepbrothers = createdf(stepbrothers)\n",
    "saw = createdf(saw)\n",
    "saw2 = createdf(saw2)\n",
    "titanic = createdf(titanic)\n",
    "piratesofthecaribbean = createdf(piratesofthecaribbean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 words to describe Star Wars\n",
      "jedi : 161\n",
      "plot : 175\n",
      "rey : 180\n",
      "this : 184\n",
      "new : 188\n",
      "trilogy : 193\n",
      "awakens : 224\n",
      "story : 224\n",
      "hope : 241\n",
      "original : 256\n",
      "great : 261\n",
      "good : 290\n",
      "it : 295\n",
      "force : 326\n",
      "characters : 357\n",
      "film : 430\n",
      "the : 662\n",
      "wars : 689\n",
      "movie : 741\n",
      "star : 800\n",
      "\n",
      "Top 20 words to describe About Time\n",
      "gleeson : 96\n",
      "rachel : 106\n",
      "curtis : 110\n",
      "places : 111\n",
      "mcadams : 119\n",
      "tim : 120\n",
      "this : 133\n",
      "about : 137\n",
      "romantic : 141\n",
      "great : 176\n",
      "good : 181\n",
      "story : 201\n",
      "it : 215\n",
      "life : 250\n",
      "travel : 265\n",
      "the : 278\n",
      "love : 342\n",
      "film : 403\n",
      "movie : 569\n",
      "time : 737\n",
      "\n",
      "Top 20 words to describe Taken\n",
      "movies : 75\n",
      "time : 86\n",
      "plot : 93\n",
      "watch : 95\n",
      "bad : 97\n",
      "thriller : 102\n",
      "fun : 104\n",
      "this : 105\n",
      "story : 106\n",
      "daughter : 139\n",
      "it : 168\n",
      "great : 187\n",
      "taken : 212\n",
      "the : 215\n",
      "good : 235\n",
      "film : 360\n",
      "liam : 405\n",
      "neeson : 470\n",
      "movie : 494\n",
      "action : 588\n",
      "\n",
      "Top 20 words to describe Toy Story\n",
      "good : 115\n",
      "movies : 126\n",
      "films : 147\n",
      "this : 149\n",
      "classic : 161\n",
      "buzz : 167\n",
      "it : 182\n",
      "toys : 191\n",
      "woody : 191\n",
      "time : 196\n",
      "characters : 203\n",
      "great : 240\n",
      "animated : 243\n",
      "animation : 276\n",
      "the : 300\n",
      "pixar : 342\n",
      "film : 504\n",
      "movie : 519\n",
      "toy : 534\n",
      "story : 612\n",
      "\n",
      "Top 20 words to describe Cloud Atlas\n",
      "movies : 83\n",
      "ambitious : 88\n",
      "actors : 97\n",
      "long : 98\n",
      "characters : 99\n",
      "book : 104\n",
      "make : 108\n",
      "great : 125\n",
      "good : 139\n",
      "this : 141\n",
      "watch : 143\n",
      "stories : 163\n",
      "cloud : 201\n",
      "atlas : 207\n",
      "time : 240\n",
      "story : 247\n",
      "it : 267\n",
      "the : 295\n",
      "film : 422\n",
      "movie : 583\n",
      "\n",
      "Top 20 words to describe Stepbrothers\n",
      "stupid : 78\n",
      "movies : 79\n",
      "watch : 91\n",
      "this : 97\n",
      "time : 104\n",
      "great : 106\n",
      "good : 111\n",
      "step : 117\n",
      "brothers : 127\n",
      "the : 130\n",
      "john : 138\n",
      "film : 143\n",
      "it : 152\n",
      "hilarious : 161\n",
      "reilly : 164\n",
      "comedy : 173\n",
      "will : 221\n",
      "ferrell : 274\n",
      "funny : 358\n",
      "movie : 429\n",
      "\n",
      "Top 20 words to describe Saw\n",
      "series : 107\n",
      "jigsaw : 108\n",
      "sequels : 108\n",
      "wan : 110\n",
      "twist : 111\n",
      "acting : 125\n",
      "movies : 125\n",
      "ending : 129\n",
      "time : 135\n",
      "story : 143\n",
      "gore : 151\n",
      "this : 155\n",
      "great : 182\n",
      "it : 218\n",
      "good : 220\n",
      "saw : 382\n",
      "the : 401\n",
      "horror : 479\n",
      "film : 522\n",
      "movie : 541\n",
      "\n",
      "Top 20 words to describe Saw 2\n",
      "this : 49\n",
      "story : 50\n",
      "love : 52\n",
      "series : 52\n",
      "watch : 52\n",
      "ii : 53\n",
      "original : 59\n",
      "gore : 60\n",
      "time : 62\n",
      "it : 64\n",
      "great : 66\n",
      "movies : 72\n",
      "jigsaw : 78\n",
      "sequel : 81\n",
      "horror : 92\n",
      "the : 129\n",
      "movie : 130\n",
      "film : 154\n",
      "good : 161\n",
      "saw : 179\n",
      "\n",
      "Top 20 words to describe Titanic\n",
      "dicaprio : 106\n",
      "people : 108\n",
      "winslet : 108\n",
      "this : 127\n",
      "romance : 130\n",
      "good : 134\n",
      "james : 163\n",
      "great : 176\n",
      "jack : 181\n",
      "it : 182\n",
      "ship : 199\n",
      "time : 211\n",
      "rose : 215\n",
      "cameron : 228\n",
      "story : 296\n",
      "love : 324\n",
      "the : 405\n",
      "film : 457\n",
      "titanic : 496\n",
      "movie : 636\n",
      "\n",
      "Top 20 words to describe Pirates of the Caribbean\n",
      "captain : 100\n",
      "pearl : 100\n",
      "time : 107\n",
      "adventure : 122\n",
      "love : 122\n",
      "it : 127\n",
      "caribbean : 128\n",
      "pirate : 137\n",
      "sparrow : 173\n",
      "action : 188\n",
      "fun : 195\n",
      "jack : 199\n",
      "great : 226\n",
      "good : 239\n",
      "pirates : 254\n",
      "johnny : 273\n",
      "film : 307\n",
      "the : 313\n",
      "depp : 340\n",
      "movie : 439\n"
     ]
    }
   ],
   "source": [
    "# Obtain the 20 most common words for all movies and print results by calling mostcommon function\n",
    "mostcommon(starwars, \"Star Wars\")\n",
    "mostcommon(abouttime, \"About Time\")\n",
    "mostcommon(taken, \"Taken\")\n",
    "mostcommon(toystory, \"Toy Story\")\n",
    "mostcommon(cloudatlas, \"Cloud Atlas\")\n",
    "mostcommon(stepbrothers, \"Stepbrothers\")\n",
    "mostcommon(saw, \"Saw\")\n",
    "mostcommon(saw2, \"Saw 2\")\n",
    "mostcommon(titanic, \"Titanic\")\n",
    "mostcommon(piratesofthecaribbean, \"Pirates of the Caribbean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all training and testing data by creating lists (various combinations of the 10 movies)\n",
    "# Basically, train1-5 and test1-5 will loop through 2 movies at a time to accomplish 5-fold Cross Validation\n",
    "# train1-10_10 and test1-10_10 will loop through each movie at a time to accomplish 10-fold Cross Validation\n",
    "\n",
    "# All dataframes in this list\n",
    "dataframes = [starwars, abouttime, taken, toystory, cloudatlas, stepbrothers, saw, saw2, titanic, piratesofthecaribbean]\n",
    "\n",
    "# Go through and exclude two movie at a time; t1 can be obtained through dataframes[0:8], t5 through dataframes[2:10]\n",
    "t2 = [starwars, abouttime, taken, toystory, cloudatlas, stepbrothers, titanic, piratesofthecaribbean]\n",
    "t3 = [starwars, abouttime, taken, toystory, saw, saw2, titanic, piratesofthecaribbean]\n",
    "t4 = [starwars, abouttime, cloudatlas, stepbrothers, saw, saw2, titanic, piratesofthecaribbean]\n",
    "\n",
    "# Go through and exclude one movie at a time; t1_10 can be obtained through dataframes[0:9], t10_10 through dataframes[1:10]\n",
    "t2_10 = [starwars, abouttime, taken, toystory, cloudatlas, stepbrothers, saw, saw2, piratesofthecaribbean]\n",
    "t3_10 = [starwars, abouttime, taken, toystory, cloudatlas, stepbrothers, saw, titanic, piratesofthecaribbean]\n",
    "t4_10 = [starwars, abouttime, taken, toystory, cloudatlas, stepbrothers, saw2, titanic, piratesofthecaribbean]\n",
    "t5_10 = [starwars, abouttime, taken, toystory, cloudatlas, saw, saw2, titanic, piratesofthecaribbean]\n",
    "t6_10 = [starwars, abouttime, taken, toystory, stepbrothers, saw, saw2, titanic, piratesofthecaribbean]\n",
    "t7_10 = [starwars, abouttime, taken, cloudatlas, stepbrothers, saw, saw2, titanic, piratesofthecaribbean]\n",
    "t8_10 = [starwars, abouttime, cloudatlas, stepbrothers, saw, saw2, titanic, piratesofthecaribbean]\n",
    "t9_10 = [starwars, taken, cloudatlas, stepbrothers, saw, saw2, titanic, piratesofthecaribbean]\n",
    "\n",
    "# Here is all training and testing data for 5-fold Cross Validation\n",
    "train1 = pd.concat(dataframes[0:8])\n",
    "test1 = pd.concat(dataframes[8:10])\n",
    "train2 = pd.concat(t2)\n",
    "test2 = pd.concat(dataframes[6:8])\n",
    "train3 = pd.concat(t3)\n",
    "test3 = pd.concat(dataframes[4:6])\n",
    "train4 = pd.concat(t4)\n",
    "test4 = pd.concat(dataframes[2:4])\n",
    "train5 = pd.concat(dataframes[2:10])\n",
    "test5 = pd.concat(dataframes[0:2])\n",
    "\n",
    "# Here is all training data for 10-fold Cross Validation, the test data will be each individual movie\n",
    "train1_10 = pd.concat(dataframes[0:9])\n",
    "train2_10 = pd.concat(t2_10)\n",
    "train3_10 = pd.concat(t3_10)\n",
    "train4_10 = pd.concat(t4_10)\n",
    "train5_10 = pd.concat(t5_10)\n",
    "train6_10 = pd.concat(t6_10)\n",
    "train7_10 = pd.concat(t7_10)\n",
    "train8_10 = pd.concat(t8_10)\n",
    "train9_10 = pd.concat(t9_10)\n",
    "train10_10 = pd.concat(dataframes[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Utilize Naive Bayes and Logistic Regression Functions and Obtain Accuracy from Each Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score using Naive Bayes 5-fold Cross Validation:\n",
      "0.8213082559732714\n",
      "\n",
      "Average accuracy score using Naive Bayes 10-fold Cross Validation:\n",
      "0.8196263076812373\n"
     ]
    }
   ],
   "source": [
    "# Call naive_bayes function and obtain accuracy scores for all iterations of 5-fold Cross Validation\n",
    "t1_5 = naive_bayes(train1, test1)\n",
    "t2_5 = naive_bayes(train2, test2)\n",
    "t3_5 = naive_bayes(train3, test3)\n",
    "t4_5 = naive_bayes(train4, test4)\n",
    "t5_5 = naive_bayes(train5, test5)\n",
    "\n",
    "t5_all = [t1_5, t2_5, t3_5, t4_5, t5_5]\n",
    "\n",
    "# Obtain the average score and print result\n",
    "score5_nb = np.mean(t5_all)\n",
    "print \"Average accuracy score using Naive Bayes 5-fold Cross Validation:\"\n",
    "print score5_nb\n",
    "\n",
    "# Call naive_bayes function and obtain accuracy scores for all iterations of 10-fold Cross Validation\n",
    "t1_10 = naive_bayes(train1_10, piratesofthecaribbean)\n",
    "t2_10 = naive_bayes(train2_10, titanic)\n",
    "t3_10 = naive_bayes(train3_10, saw2)\n",
    "t4_10 = naive_bayes(train4_10, saw)\n",
    "t5_10 = naive_bayes(train5_10, stepbrothers)\n",
    "t6_10 = naive_bayes(train6_10, cloudatlas)\n",
    "t7_10 = naive_bayes(train7_10, toystory)\n",
    "t8_10 = naive_bayes(train8_10, taken)\n",
    "t9_10 = naive_bayes(train9_10, abouttime)\n",
    "t10_10 = naive_bayes(train10_10, starwars)\n",
    "\n",
    "t10_all = [t1_10, t2_10, t3_10, t4_10, t5_10, t6_10, t7_10, t8_10, t9_10, t10_10]\n",
    "\n",
    "# Obtain the average score and print result\n",
    "score10_nb = np.mean(t10_all)\n",
    "print \"\\nAverage accuracy score using Naive Bayes 10-fold Cross Validation:\"\n",
    "print score10_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score using Logistic Regression 5-fold Cross Validation:\n",
      "0.8213082559732714\n",
      "\n",
      "Average accuracy score using Logistic Regression 10-fold Cross Validation:\n",
      "0.8185657023272654\n"
     ]
    }
   ],
   "source": [
    "# Call log_reg function and obtain accuracy scores for all iterations of 5-fold Cross Validation\n",
    "t1 = log_reg(train1, test1)\n",
    "t2 = log_reg(train2, test2)\n",
    "t3 = log_reg(train3, test3)\n",
    "t4 = log_reg(train4, test4)\n",
    "t5 = log_reg(train5, test5)\n",
    "\n",
    "t5_all = [t1_5, t2_5, t3_5, t4_5, t5_5]\n",
    "\n",
    "# Obtain the average score and print result\n",
    "score5_log = np.mean(t5_all)\n",
    "print \"Average accuracy score using Logistic Regression 5-fold Cross Validation:\"\n",
    "print score5_log\n",
    "\n",
    "# Call log_reg function and obtain accuracy scores for all iterations of 10-fold Cross Validation\n",
    "t1_10 = log_reg(train1_10, piratesofthecaribbean)\n",
    "t2_10 = log_reg(train2_10, titanic)\n",
    "t3_10 = log_reg(train3_10, saw2)\n",
    "t4_10 = log_reg(train4_10, saw)\n",
    "t5_10 = log_reg(train5_10, stepbrothers)\n",
    "t6_10 = log_reg(train6_10, cloudatlas)\n",
    "t7_10 = log_reg(train7_10, toystory)\n",
    "t8_10 = log_reg(train8_10, taken)\n",
    "t9_10 = log_reg(train9_10, abouttime)\n",
    "t10_10 = log_reg(train10_10, starwars)\n",
    "\n",
    "t10_all = [t1_10, t2_10, t3_10, t4_10, t5_10, t6_10, t7_10, t8_10, t9_10, t10_10]\n",
    "\n",
    "# Obtain the average score and print result\n",
    "score10_log = np.mean(t10_all)\n",
    "print \"\\nAverage accuracy score using Logistic Regression 10-fold Cross Validation:\"\n",
    "print score10_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: Report all results in a table and a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Results of Naive Bayes and Logistic Regression using 5-fold and 10-fold Cross Validation:\n",
      "\n",
      "         Naive Bayes  Logistic Regression\n",
      "5-fold      0.821308             0.821308\n",
      "10-fold     0.819626             0.818566\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe containing the results and print it out\n",
    "s5 = np.array([score5_nb, score5_log])\n",
    "s10 = np.array([score10_nb, score10_log])\n",
    "index = np.array([\"5-fold\", \"10-fold\"])\n",
    "results = pd.DataFrame([s5, s10], columns=['Naive Bayes', 'Logistic Regression'], index=index)\n",
    "print \"\\nAccuracy Results of Naive Bayes and Logistic Regression using 5-fold and 10-fold Cross Validation:\\n\"\n",
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb920c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use matplotlib.pyplot to print the results of all four tests\n",
    "# It can be observed that the results are all very close to each other\n",
    "x = [1, 2, 3, 4]\n",
    "data = [score5_nb, score5_log, score10_nb, score10_log]\n",
    "\n",
    "plt.bar(x, data, align='center')\n",
    "plt.title(\"All Accuracy Results\")\n",
    "plt.xlabel(\"Test\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.axis((0.5, 4.5, 0, 1))\n",
    "plt.xticks(range(1, 5), ('NB 5', 'Log 5', 'NB 10', 'Log 10'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
